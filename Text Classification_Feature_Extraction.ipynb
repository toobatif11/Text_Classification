{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sklearn.feature_extraction.text.CountVectorizer\n",
    "\n",
    "CountVectorizer class Convert a collection of text documents to a matrix of token counts\n",
    "\n",
    "min_df : float in range [0.0, 1.0] or int, default=1\n",
    "When building the vocabulary ignore terms that have a document frequency strictly lower than the given threshold. \n",
    "This value is also called cut-off in the literature.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['about', 'and', 'document', 'first', 'is', 'one', 'python', 'second', 'the', 'third', 'this']\n",
      "[[0 0 1 1 1 0 0 0 1 0 1]\n",
      " [0 0 2 0 1 0 0 1 1 0 1]\n",
      " [1 1 0 0 1 1 1 0 1 1 1]\n",
      " [0 0 1 1 1 0 0 0 1 0 1]]\n",
      "  (0, 2)\t1\n",
      "  (0, 3)\t1\n",
      "  (0, 8)\t1\n",
      "  (0, 4)\t1\n",
      "  (0, 10)\t1\n",
      "  (1, 7)\t1\n",
      "  (1, 2)\t2\n",
      "  (1, 8)\t1\n",
      "  (1, 4)\t1\n",
      "  (1, 10)\t1\n",
      "  (2, 6)\t1\n",
      "  (2, 0)\t1\n",
      "  (2, 5)\t1\n",
      "  (2, 9)\t1\n",
      "  (2, 1)\t1\n",
      "  (2, 8)\t1\n",
      "  (2, 4)\t1\n",
      "  (2, 10)\t1\n",
      "  (3, 2)\t1\n",
      "  (3, 3)\t1\n",
      "  (3, 8)\t1\n",
      "  (3, 4)\t1\n",
      "  (3, 10)\t1\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import pandas as pd\n",
    "corpus = [\n",
    "    'This is the first document.',\n",
    "    'This document is the second document.',\n",
    "    'And this is the third one about python.',\n",
    "    'Is this the first document?',\n",
    "]\n",
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(corpus)#Learn the vocabulary dictionary and return term-document matrix(matrix of term count)\n",
    "\n",
    "feature_names= vectorizer.get_feature_names()\n",
    "print(feature_names)\n",
    "\n",
    "print(X.toarray())  \n",
    "print(X) # sparse matrix is the one having zero entries ,occupy less space because space is not reserved for zero matrix entries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   about  and  document  first  is  one  python  second  the  third  this\n",
      "0      0    0         1      1   1    0       0       0    1      0     1\n",
      "1      0    0         2      0   1    0       0       1    1      0     1\n",
      "2      1    1         0      0   1    1       1       0    1      1     1\n",
      "3      0    0         1      1   1    0       0       0    1      0     1\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "df = pd.DataFrame(data=np.array(X.toarray()),columns=feature_names)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 1 0 1 0 1 0 1]\n",
      " [1 1 1 0 2 0 2 0 0]\n",
      " [0 1 1 0 1 0 1 1 1]\n",
      " [0 0 1 1 0 1 0 0 0]]\n",
      "\n",
      "[[0 0 1 0 0 0 1 0 0]]\n",
      "\n",
      "\n",
      "   and  beautiful  blue  cheese  is  love  sky  so  the\n",
      "0    0          0     1       0   1     0    1   0    1\n",
      "1    1          1     1       0   2     0    2   0    0\n",
      "2    0          1     1       0   1     0    1   1    1\n",
      "3    0          0     1       1   0     1    0   0    0\n",
      "\n",
      "   and  beautiful  blue  cheese  is  love  sky  so  the\n",
      "0    0          0     1       0   0     0    1   0    0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    " # training corpus\n",
    "CORPUS = [   \n",
    "'the sky is blue',\n",
    "'sky is blue and sky is beautiful',\n",
    "'the beautiful sky is so blue',\n",
    "'i love blue cheese'\n",
    "]\n",
    "\n",
    "# the sky, sky is , is blue\n",
    "# sky is , is blue, blue and, and sky is, is beautiful\n",
    "# the beautiful,beautiful sky, sky is, is so, so blue\n",
    "# i love, love blue, blue cheese\n",
    "\n",
    "# test document\n",
    "new_doc = ['loving this blue sky today']\n",
    "\n",
    " # 1 gram or individaul words only \n",
    "#try ngram_range(1,2)for 1 and 2 grams and (1,3), (2,2)  for 2 grams only -bag of bigrams, (3,3) for 3 grams only-bag of trigrams\n",
    "\n",
    "\n",
    "def bow_extractor(corpus, ngram_range=(1,1)):\n",
    "    \n",
    "    vectorizer = CountVectorizer(min_df=1, ngram_range=ngram_range)\n",
    "    #with min_df set to 1 to take terms having a minimum frequency of 1 in the overall document vector space.\n",
    "    \n",
    "    features = vectorizer.fit_transform(corpus) ##Learn the vocabulary dictionary and return term-document matrix.\n",
    "    return vectorizer, features\n",
    "\n",
    "# build bow vectorizer and get features\n",
    "\n",
    "bow_vectorizer, bow_features = bow_extractor(CORPUS)\n",
    "features = bow_features.todense() #Return a dense matrix representation of this matrix.\n",
    "\n",
    "feature_names=bow_vectorizer.get_feature_names()\n",
    "print (features)\n",
    "print()\n",
    "'''features2 = bow_features.toarray()\n",
    "print (features2'''\n",
    "\n",
    "# extract features from new document using built vectorizer\n",
    "new_doc_features = bow_vectorizer.transform(new_doc)\n",
    "new_doc_features = new_doc_features.todense()\n",
    "new_doc_feature_names=bow_vectorizer.get_feature_names()\n",
    "print (new_doc_features)\n",
    "print()\n",
    "print()\n",
    "df = pd.DataFrame(data=np.array(features),columns=feature_names)\n",
    "print(df)\n",
    "print()\n",
    "df = pd.DataFrame(data=np.array(new_doc_features),columns=new_doc_feature_names)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'sky is', 'the beautiful', 'beautiful sky', 'and sky', 'is so', 'is beautiful', 'love blue', 'so blue', 'blue cheese', 'is blue', 'blue and', 'i love'}\n",
      "12\n"
     ]
    }
   ],
   "source": [
    "s= set( ['sky is' , 'is blue', 'blue and', 'and sky','sky is', 'is beautiful','the beautiful','beautiful sky', 'sky is', 'is so', 'so blue','i love', 'love blue', 'blue cheese'])\n",
    "print(s)\n",
    "print(len(s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 0 0 1 0 0 1 0 0 1]\n",
      " [1 0 1 0 1 1 0 0 2 0 0 0]\n",
      " [0 1 0 0 0 0 1 0 1 1 1 0]\n",
      " [0 0 0 1 0 0 0 1 0 0 0 0]]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-33-892fec624d9a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[0mfeature_names\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbow_vectorizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_feature_names\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[0mprint\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 34\u001b[1;33m \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfeature_names\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     35\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "import numpy as np\n",
    "\n",
    " # training corpus\n",
    "CORPUS = [   \n",
    "'the sky is blue',\n",
    "'sky is blue and sky is beautiful',\n",
    "'the beautiful sky is so blue',\n",
    "'i love blue cheese'\n",
    "]\n",
    "# test document\n",
    "new_doc = ['loving this blue sky today']\n",
    "\n",
    " # 1 gram or individaul words only \n",
    "#try ngram_range(1,2)for 1 and 2 grams and (1,3), (2,2)  for 2 grams only -bag of bigrams, (3,3) for 3 grams only-bag of trigrams\n",
    "\n",
    "\n",
    "def bow_extractor(corpus, ngram_range=(2,2)):\n",
    "    \n",
    "    vectorizer = CountVectorizer(min_df=1, ngram_range=ngram_range)\n",
    "    #with min_df set to 1 to take terms having a minimum frequency of 1 in the overall document vector space.\n",
    "    \n",
    "    features = vectorizer.fit_transform(corpus) ##Learn the vocabulary dictionary and return term-document matrix.\n",
    "    return vectorizer, features\n",
    "\n",
    "# build bow vectorizer and get features\n",
    "\n",
    "bow_vectorizer, bow_features = bow_extractor(CORPUS)\n",
    "features = bow_features.todense() #Return a dense matrix representation of this matrix.\n",
    "\n",
    "feature_names=bow_vectorizer.get_feature_names()\n",
    "print (features)\n",
    "df = pd.DataFrame(data=np.array(features),columns=feature_names)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'TF' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-65-f50e9fd4dce8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mTF\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mIDF\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'TF' is not defined"
     ]
    }
   ],
   "source": [
    "TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "corpus = [\n",
    "    'This is the first document.',\n",
    "    'This document is the second document.',\n",
    "    'And this is the third one.',\n",
    "    'Is this the first document?',\n",
    "]\n",
    "new_doc = ['loving this blue sky today']\n",
    "\n",
    "def display_features(features, feature_names):\n",
    "    \n",
    "    df = pd.DataFrame(data=features, columns=feature_names)\n",
    "    print (df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 1 0 1 0 1 0 1]\n",
      " [1 1 1 0 2 0 2 0 0]\n",
      " [0 1 1 0 1 0 1 1 1]\n",
      " [0 0 1 1 0 1 0 0 0]]\n",
      "\n",
      "\n",
      "   and  beautiful  blue  cheese  is  love  sky  so  the\n",
      "0    0          0     1       0   1     0    1   0    1\n",
      "1    1          1     1       0   2     0    2   0    0\n",
      "2    0          1     1       0   1     0    1   1    1\n",
      "3    0          0     1       1   0     1    0   0    0\n",
      "[[0 0 1 0 0 0 1 0 0]]\n",
      "\n",
      "\n",
      "['and', 'beautiful', 'blue', 'cheese', 'is', 'love', 'sky', 'so', 'the']\n"
     ]
    }
   ],
   "source": [
    "CORPUS = [\n",
    "'the sky is blue',\n",
    "'sky is blue and sky is beautiful',\n",
    "'the beautiful sky is so blue',\n",
    "'i love blue cheese'\n",
    "]\n",
    "new_doc = ['loving this blue sky today']\n",
    "def display_features(features, feature_names):\n",
    "    \n",
    "    df = pd.DataFrame(data=features, columns=feature_names)\n",
    "    print (df)\n",
    "\n",
    "def bow_extractor(corpus, ngram_range=(1,1)): # 1 gram or individaul words only\n",
    "    # try ngram_range(1,2)for 1 and 2 grams and (1,3), (2,2)  for 2 grams only (3,3) for 3 grams only\n",
    "    \n",
    "    vectorizer = CountVectorizer(min_df=1, ngram_range=ngram_range)#with min_df set to 1 to take terms having a minimum frequency of 1 in the overall document vector space.\n",
    "    features = vectorizer.fit_transform(corpus) ##Learn the vocabulary dictionary and return term-document matrix.\n",
    "    return vectorizer, features\n",
    "    \n",
    "bow_vectorizer, bow_features = bow_extractor(CORPUS)\n",
    "features = bow_features.todense()\n",
    "print (features)\n",
    "feature_names = bow_vectorizer.get_feature_names()\n",
    "print()\n",
    "print()\n",
    "display_features(bow_features.todense(),feature_names)\n",
    "\n",
    "new_doc_features = bow_vectorizer.transform(new_doc)\n",
    "new_doc_features = new_doc_features.todense()\n",
    "print (new_doc_features)\n",
    "print()\n",
    "print()\n",
    "\n",
    "feature_names = bow_vectorizer.get_feature_names()\n",
    "print (feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   and  beautiful  blue  cheese  is  love  sky  so  the\n",
      "0    0          0     1       0   1     0    1   0    1\n",
      "1    1          1     1       0   2     0    2   0    0\n",
      "2    0          1     1       0   1     0    1   1    1\n",
      "3    0          0     1       1   0     1    0   0    0\n",
      "\n",
      "   and  beautiful  blue  cheese  is  love  sky  so  the\n",
      "0    0          0     1       0   0     0    1   0    0\n"
     ]
    }
   ],
   "source": [
    "display_features(features, feature_names)\n",
    "print()\n",
    "display_features(new_doc_features, feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TfidfTransformer(\n",
    "    [\"norm='l2'\", 'use_idf=True', 'smooth_idf=True', 'sublinear_tf=False'],\n",
    ")\n",
    "Docstring:     \n",
    "Transform a count matrix to a normalized tf or tf-idf representation\n",
    "\n",
    "Tf means term-frequency while tf-idf means term-frequency times inverse\n",
    "document-frequency. This is a common term weighting scheme in information\n",
    "retrieval, that has also found good use in document classification.\n",
    "\n",
    "The goal of using tf-idf instead of the raw frequencies of occurrence of a\n",
    "token in a given document is to scale down(reduce the size of something) the impact of tokens that occur\n",
    "very frequently in a given corpus and that are hence empirically less\n",
    "informative than features that occur in a small fraction of the training\n",
    "corpus.\n",
    "\n",
    "The formula that is used to compute the tf-idf for a term t of a document d\n",
    "in a document set is tf-idf(t, d) = tf(t, d) * idf(t), and the idf is\n",
    "computed as idf(t) = log [ n / df(t) ] + 1 (if ``smooth_idf=False``), where\n",
    "n is the total number of documents in the document set and df(t) is the\n",
    "document frequency of t; the document frequency is the number of documents\n",
    "in the document set that contain the term t. The effect of adding \"1\" to\n",
    "the idf in the equation above is that terms with zero idf, i.e., terms\n",
    "that occur in all documents in a training set, will not be entirely\n",
    "ignored.\n",
    "(Note that the idf formula above differs from the standard textbook\n",
    "notation that defines the idf as\n",
    "idf(t) = log [ n / (df(t) + 1) ]).\n",
    "\n",
    "If ``smooth_idf=True`` (the default), the constant \"1\" is added to the\n",
    "numerator and denominator of the idf as if an extra document was seen\n",
    "containing every term in the collection exactly once, which prevents\n",
    "zero divisions: idf(d, t) = log [ (1 + n) / (1 + df(d, t)) ] + 1.\n",
    "\n",
    "Furthermore, the formulas used to compute tf and idf depend\n",
    "on parameter settings that correspond to the SMART notation used in IR\n",
    "as follows:\n",
    "\n",
    "Tf is \"n\" (natural) by default, \"l\" (logarithmic) when\n",
    "``sublinear_tf=True``.\n",
    "Idf is \"t\" when use_idf is given, \"n\" (none) otherwise.\n",
    "Normalization is \"c\" (cosine) when ``norm='l2'``, \"n\" (none)\n",
    "when ``norm=None``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    and  beautiful  blue  cheese    is  love   sky    so   the\n",
      "0  0.00       0.00  0.40    0.00  0.49  0.00  0.49  0.00  0.60\n",
      "1  0.44       0.35  0.23    0.00  0.56  0.00  0.56  0.00  0.00\n",
      "2  0.00       0.43  0.29    0.00  0.35  0.00  0.35  0.55  0.43\n",
      "3  0.00       0.00  0.35    0.66  0.00  0.66  0.00  0.00  0.00\n"
     ]
    }
   ],
   "source": [
    "#getting the tfidf-based feature vectors, considering we have our Bag of Words feature vectors.\n",
    "#the TfidfTransformer class, which helps us in computing the tfidfs for each document\n",
    "\n",
    "'''In our implementation we will be adding 1 to the document frequency for each term just to indicate that we also have one more \n",
    "document in our corpus that essentially has every term in the vocabulary. \n",
    "This is to prevent potential division-by-zero errors and smoothen the inverse document frequencies. \n",
    "We also add 1 to the result of our idf computation to avoid ignoring terms completely that might have zero idf.\n",
    "\n",
    "The final TF-IDF metric we will be using is a normalized version of the tfidf matrix we get from the product of tf and idf. \n",
    "We will normalize the tfidf matrix by dividing it with the L2 norm of the matrix, also known as the Euclidean norm, \n",
    "which is the square root of the sum of the square of each term’s tfidf weight. \n",
    "Mathematically we can represent the final tfidf feature vector as tfidf = tfidf /||tfidf||\n",
    "\n",
    "\n",
    "\n",
    "smooth_idf : boolean (default=True)\n",
    "    Smooth idf weights by adding one to document frequencies, as if an\n",
    "    extra document was seen containing every term in the collection\n",
    "    exactly once. Prevents zero divisions.\n",
    "        \n",
    "use_idf : boolean (default=True)\n",
    "Enable inverse-document-frequency reweighting.\n",
    "\n",
    "\n",
    "'''\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "def tfidf_transformer(bow_matrix):\n",
    "    \n",
    "    transformer = TfidfTransformer(norm='l2',\n",
    "                                   smooth_idf=True,\n",
    "                                   use_idf=True)\n",
    "    tfidf_matrix = transformer.fit_transform(bow_matrix)\n",
    "    return transformer, tfidf_matrix\n",
    "\n",
    "# build tfidf transformer and show train corpus tfidf features\n",
    "feature_names = bow_vectorizer.get_feature_names()\n",
    "\n",
    "tfidf_trans, tdidf_features = tfidf_transformer(bow_features) # pass Bag of Words feature vectors.\n",
    "\n",
    "features = np.round(tdidf_features.todense(), 2)\n",
    "\n",
    "display_features(np.round(tdidf_features.todense(), 2), feature_names)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    and  beautiful  blue  cheese    is  love   sky    so   the\n",
      "0  0.00       0.00   1.0    0.00  1.22  0.00  1.22  0.00  1.51\n",
      "1  1.92       1.51   1.0    0.00  2.45  0.00  2.45  0.00  0.00\n",
      "2  0.00       1.51   1.0    0.00  1.22  0.00  1.22  1.92  1.51\n",
      "3  0.00       0.00   1.0    1.92  0.00  1.92  0.00  0.00  0.00\n"
     ]
    }
   ],
   "source": [
    "# compute TF-IDF matrix using TfidfTransformer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "def tfidf_transformer(bow_matrix):\n",
    "    \n",
    "    transformer = TfidfTransformer(norm=None,\n",
    "                                   smooth_idf=True,\n",
    "                                   use_idf=True)\n",
    "    tfidf_matrix = transformer.fit_transform(bow_matrix)\n",
    "    return transformer, tfidf_matrix\n",
    "\n",
    "# build tfidf transformer and show train corpus tfidf features\n",
    "feature_names = bow_vectorizer.get_feature_names()\n",
    "\n",
    "tfidf_trans, tdidf_features = tfidf_transformer(bow_features) # pass Bag of Words feature vectors.\n",
    "\n",
    "features = np.round(tdidf_features.todense(), 2)\n",
    "\n",
    "display_features(features, feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.   0.   1.   0.   1.22 0.   1.22 0.   1.51]\n"
     ]
    }
   ],
   "source": [
    "tmp_matrix=np.round(tdidf_features.toarray(),2)\n",
    "print(tmp_matrix[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.22\n",
      "0.49\n"
     ]
    }
   ],
   "source": [
    "N=4\n",
    "DF=3\n",
    "tf=np.round(1+np.log((1+N)/(1+DF)),2)\n",
    "print(tf)\n",
    "tf=np.round(tf/np.sqrt(np.sum(np.square(tmp_matrix[0]))),2)\n",
    "print(tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.51\n",
      "0.58\n"
     ]
    }
   ],
   "source": [
    "N=4\n",
    "DF=2\n",
    "tf=np.round(1+np.log((1+N)/(1+DF)),2)\n",
    "print(tf)\n",
    "tf=np.round(tf/np.sqrt(np.sum(np.square(tmp_matrix[0]))),2)\n",
    "print(tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.92\n",
      "0.74\n"
     ]
    }
   ],
   "source": [
    "N=4\n",
    "DF=1\n",
    "tf=np.round(1+np.log((1+N)/(1+DF)),2)\n",
    "print(tf)\n",
    "tf=np.round(tf/np.sqrt(np.sum(np.square(tmp_matrix[0]))),2)\n",
    "print(tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   and  beautiful  blue  cheese   is  love   sky   so  the\n",
      "0  0.0        0.0  0.63     0.0  0.0   0.0  0.77  0.0  0.0\n"
     ]
    }
   ],
   "source": [
    "#show tfidf features for new_doc using built tfidf transformer\n",
    "nd_tfidf = tfidf_trans.transform(new_doc_features)\n",
    "nd_features = np.round(nd_tfidf.todense(), 2)\n",
    "display_features(nd_features, feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   and  beautiful  blue  cheese   is  love  sky   so  the\n",
      "0  0.0        0.0   1.0     0.0  1.0   0.0  1.0  0.0  1.0\n",
      "1  1.0        1.0   1.0     0.0  2.0   0.0  2.0  0.0  0.0\n",
      "2  0.0        1.0   1.0     0.0  1.0   0.0  1.0  1.0  1.0\n",
      "3  0.0        0.0   1.0     1.0  0.0   1.0  0.0  0.0  0.0\n"
     ]
    }
   ],
   "source": [
    "#Compute TFiDF matrix your own, first find term freq,then  compute our document frequencies (DF) for each term based on the\n",
    "#number of documents in which it occurs from bag of words feature matrix, now we have document frequencies, \n",
    "#we will compute the inverse document frequency (idf), finally compute the tfidf feature matrix using matrix multiplication\n",
    "#\n",
    "# show term frequencies from bag of words matrix\n",
    "\n",
    "import scipy.sparse as sp\n",
    "from numpy.linalg import norm\n",
    "feature_names = bow_vectorizer.get_feature_names()\n",
    "\n",
    "# compute term frequency\n",
    "tf = bow_features.todense()\n",
    "tf = np.array(tf, dtype='float64')\n",
    "\n",
    "# show term frequencies\n",
    "display_features(tf, feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   and  beautiful  blue  cheese  is  love  sky  so  the\n",
      "0    2          3     5       2   4     2    4   2    3\n"
     ]
    }
   ],
   "source": [
    "# build the document frequency matrix\n",
    "\n",
    "df = np.diff(sp.csc_matrix(bow_features, copy=True).indptr)\n",
    "df = 1 + df # to smoothen idf later\n",
    "# show document frequencies\n",
    "display_features([df], feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    and  beautiful  blue  cheese    is  love   sky    so   the\n",
      "0  1.92       1.51   1.0    1.92  1.22  1.92  1.22  1.92  1.51\n",
      "(9,)\n"
     ]
    }
   ],
   "source": [
    "# compute inverse document frequencies\n",
    "total_docs = 1 + len(CORPUS)\n",
    "idf = 1.0 + np.log(float(total_docs) / df)\n",
    "\n",
    "# show inverse document frequencies\n",
    "display_features([np.round(idf, 2)], feature_names)\n",
    "print(idf.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    and  beautiful  blue  cheese    is  love   sky    so   the\n",
      "0  0.00       0.00   1.0    0.00  1.22  0.00  1.22  0.00  1.51\n",
      "1  1.92       1.51   1.0    0.00  2.45  0.00  2.45  0.00  0.00\n",
      "2  0.00       1.51   1.0    0.00  1.22  0.00  1.22  1.92  1.51\n",
      "3  0.00       0.00   1.0    1.92  0.00  1.92  0.00  0.00  0.00\n"
     ]
    }
   ],
   "source": [
    "# compute tfidf feature matrix\n",
    "tfidf = tf * idf\n",
    "# show tfidf feature matrix\n",
    "display_features(np.round(tfidf, 2), feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "computes the tfidf norms for each document and then divides the tfidf weights\n",
    "with the norm to give us the final desired tfidf matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2.5  4.35 3.5  2.89]\n"
     ]
    }
   ],
   "source": [
    "# compute L2 norms # square of sum of each squared term\n",
    "norms = norm(tfidf, axis=1)\n",
    "# print norms for each document\n",
    "print(np.round(norms, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    and  beautiful  blue  cheese    is  love   sky    so   the\n",
      "0  0.00       0.00  0.40    0.00  0.49  0.00  0.49  0.00  0.60\n",
      "1  0.44       0.35  0.23    0.00  0.56  0.00  0.56  0.00  0.00\n",
      "2  0.00       0.43  0.29    0.00  0.35  0.00  0.35  0.55  0.43\n",
      "3  0.00       0.00  0.35    0.66  0.00  0.66  0.00  0.00  0.00\n"
     ]
    }
   ],
   "source": [
    "# compute normalized tfidf\n",
    "norm_tfidf = tfidf / norms[:, None]\n",
    "# show final tfidf feature matrix\n",
    "display_features(np.round(norm_tfidf, 2), feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2.50494598 4.35010407 3.49707286 2.88865719]\n",
      "[2.50494598 4.35010407 3.49707286 2.88865719]\n",
      "1\n",
      "ddaa [[2.50494598]\n",
      " [4.35010407]\n",
      " [3.49707286]\n",
      " [2.88865719]]\n",
      "ddd [2.50494598 4.35010407 3.49707286 2.88865719]\n",
      "2\n",
      "(4, 1)\n",
      "[[1]\n",
      " [2]\n",
      " [3]\n",
      " [4]\n",
      " [5]]\n"
     ]
    }
   ],
   "source": [
    "print(norms)\n",
    "print(norms[:])\n",
    "print(norms.ndim)\n",
    "print('ddaa',norms[:,None])\n",
    "print('ddd',np.transpose(norms[:]))\n",
    "print(norms[:,None].ndim)\n",
    "print(norms[:,None].shape)\n",
    "a=np.array([1,2,3,4,5])\n",
    "print(a[:,None])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "#implement a genericfunction that can directly compute the tfidf-based feature vectors for documents from the\n",
    "#raw documents themselves.function makes use of the TfidfVectorizer, which directly computes\n",
    "#the tfidf vectors by taking the raw documents themselves as input and internally\n",
    "#computing the term frequencies as well as the inverse document frequencies, eliminating\n",
    "#the need to use the CountVectorizer for computing the term frequencies based on the\n",
    "#Bag of Words model. Support is also present for adding n-grams to the feature vectors. \n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "def tfidf_extractor(corpus, ngram_range=(1,1)):\n",
    "\n",
    "    vectorizer = TfidfVectorizer(min_df=1,norm='l2',smooth_idf=True,use_idf=True,ngram_range=ngram_range)\n",
    "\n",
    "    features = vectorizer.fit_transform(corpus)\n",
    "\n",
    "    return vectorizer, features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    and  beautiful  blue  cheese    is  love   sky    so   the\n",
      "0  0.00       0.00  0.40    0.00  0.49  0.00  0.49  0.00  0.60\n",
      "1  0.44       0.35  0.23    0.00  0.56  0.00  0.56  0.00  0.00\n",
      "2  0.00       0.43  0.29    0.00  0.35  0.00  0.35  0.55  0.43\n",
      "3  0.00       0.00  0.35    0.66  0.00  0.66  0.00  0.00  0.00\n"
     ]
    }
   ],
   "source": [
    "# build tfidf vectorizer and get training corpus feature vectors\n",
    "tfidf_vectorizer, tdidf_features = tfidf_extractor(CORPUS)\n",
    "display_features(np.round(tdidf_features.todense(), 2), feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   and  beautiful  blue  cheese   is  love   sky   so  the\n",
      "0  0.0        0.0  0.63     0.0  0.0   0.0  0.77  0.0  0.0\n"
     ]
    }
   ],
   "source": [
    "# get tfidf feature vector for the new document\n",
    "nd_tfidf = tfidf_vectorizer.transform(new_doc)\n",
    "display_features(np.round(nd_tfidf.todense(), 2), feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 0 4]\n",
      " [0 0 5]\n",
      " [2 3 6]]\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'tuple' object has no attribute 'indptr'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-113-657ffc9091da>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m4\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m6\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcsc_matrix\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcol\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcsc_matrix\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcol\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: 'tuple' object has no attribute 'indptr'"
     ]
    }
   ],
   "source": [
    "column i are stored in indices[indptr[i]:indptr[i+1]]\n",
    "coloumn 0               indices[indptr[0]:indptr[1]]  # indices[0,2]\n",
    "column 1                indices[indptr[1]:indptr[2]]  # indices[2,3]\n",
    "column 2                 indices[indptr[2]:indptr[3]] #indices[3,6]\n",
    "\n",
    "\n",
    "row = np.array([0, 2, 2, 0, 1, 2])\n",
    "col = np.array([0, 0, 1, 2, 2, 2])\n",
    "data = np.array([1, 2, 3, 4, 5, 6])\n",
    "print(sp.csc_matrix((data, (row, col)), shape=(3, 3)).toarray())\n",
    "print(sp.csc_matrix((data, (row, col)), shape=(3, 3).indptr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "There are various approaches to creating more advanced word vectorization models for\n",
    "extracting features from text data. Here we will discuss a couple of them that use Google’s\n",
    "popular word2vec algorithm. The word2vec model, released in 2013 by Google, is a neural\n",
    "network–based implementation that learns distributed vector representations of words\n",
    "based on continuous Bag of Words and skip-gram–based architectures. \n",
    "The word2vec framework is much faster than other neural network–based implementations and does\n",
    "not require manual labels to create meaningful representations among words. You can\n",
    "find more details on Google’s word2vec project at https://code.google.com/archive/p/\n",
    "word2vec/. You can even try out some of the implementations yourself if you are interested.\n",
    "We will be using the gensim library in our implementation, which is Python\n",
    "implementation for word2vec that provides several high-level interfaces for easily building\n",
    "these models. The basic idea is to provide a corpus of documents as input and get feature\n",
    "vectors for them as output. Internally, it constructs a vocabulary based on the input text\n",
    "documents and learns vector representations for words based on various techniques\n",
    "mentioned earlier, and once this is complete, it builds a model that can be used to\n",
    "extract word vectors for each word in a document. Using various techniques like average\n",
    "weighting or tfidf weighting, we can compute the averaged vector representation of a\n",
    "document using its word vectors. You can get more details about the interface for gensim‘s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conda install gensim\n",
    "The word2vec tool takes a text corpus as input and produces the word vectors as output.\n",
    "It first constructs a vocabulary from the training text data and then learns vector representation of words. \n",
    "The resulting word vector file can be used as features in many natural language processing and machine learning applications.\n",
    "\n",
    "A simple way to investigate the learned representations is to find the closest words for a user-specified word. \n",
    "The distance tool serves that purpose. For example, if you enter 'france', distance will display the most similar words and their distances to 'france', which should look like:\n",
    " https://www.kdnuggets.com/2018/04/robust-word2vec-models-gensim.html\n",
    "    https://skymind.ai/wiki/word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['sky', 'is', 'blue', 'the', 'beautiful']\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "import nltk\n",
    " # training corpus\n",
    "CORPUS = [   \n",
    "'the sky is blue',\n",
    "'sky is blue and sky is beautiful',\n",
    "'the beautiful sky is so blue',\n",
    "'i love blue cheese'\n",
    "]\n",
    "# test document\n",
    "new_doc = ['loving this blue sky today']\n",
    "\n",
    "TOKENIZED_CORPUS = [nltk.word_tokenize(sentence) \n",
    "                    for sentence in CORPUS]\n",
    "tokenized_new_doc = [nltk.word_tokenize(sentence) \n",
    "                    for sentence in new_doc]                        \n",
    "\n",
    "model = gensim.models.Word2Vec(TOKENIZED_CORPUS, \n",
    "                               size=10,\n",
    "                               window=10,\n",
    "                               min_count=2,\n",
    "                               sample=1e-3)\n",
    "\n",
    "\n",
    "print(model.wv.index2word)\n",
    "#print(model['the'].)\n",
    "#for i in TOKENIZED_CORPUS:\n",
    "    #print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "word is  the\n",
      "\n",
      "After word the and feature vector [ 0.04696456  0.04587301 -0.03998681 -0.04094198 -0.04281063 -0.03340809\n",
      "  0.01781495  0.02247143  0.03157189  0.03632472] \n",
      "\n",
      "After addition word the and feature vector [ 0.04696456  0.04587301 -0.03998681 -0.04094198 -0.04281063 -0.03340809\n",
      "  0.01781495  0.02247143  0.03157189  0.03632472] \n",
      "\n",
      "word is  sky\n",
      "\n",
      "After word sky and feature vector [ 0.04317668 -0.0389986   0.01506065 -0.03560273 -0.00640112 -0.00782156\n",
      "  0.00584124 -0.00672364 -0.02584449 -0.02584974] \n",
      "\n",
      "After addition word sky and feature vector [ 0.09014125  0.00687441 -0.02492616 -0.07654471 -0.04921175 -0.04122965\n",
      "  0.02365619  0.01574779  0.0057274   0.01047499] \n",
      "\n",
      "word is  is\n",
      "\n",
      "After word is and feature vector [-0.02266192 -0.00251063 -0.03345118 -0.04034328  0.00706533 -0.02801719\n",
      "  0.03403306 -0.04139802  0.01580966  0.00674342] \n",
      "\n",
      "After addition word is and feature vector [ 0.06747933  0.00436378 -0.05837735 -0.11688799 -0.04214642 -0.06924684\n",
      "  0.05768925 -0.02565023  0.02153706  0.0172184 ] \n",
      "\n",
      "word is  blue\n",
      "\n",
      "After word blue and feature vector [-0.0284033   0.01511517  0.01588725 -0.04928996  0.03953237 -0.01278873\n",
      " -0.02903343  0.02796526  0.04616746 -0.03628164] \n",
      "\n",
      "After addition word blue and feature vector [ 0.03907602  0.01947895 -0.04249009 -0.16617795 -0.00261405 -0.08203557\n",
      "  0.02865582  0.00231503  0.06770452 -0.01906324] \n",
      "hello\n",
      "\n",
      "\n",
      "finalized feature vector [ 0.00976901  0.00486974 -0.01062252 -0.04154449 -0.00065351 -0.02050889\n",
      "  0.00716396  0.00057876  0.01692613 -0.00476581]\n",
      "\n",
      "word is  sky\n",
      "\n",
      "After word sky and feature vector [ 0.04317668 -0.0389986   0.01506065 -0.03560273 -0.00640112 -0.00782156\n",
      "  0.00584124 -0.00672364 -0.02584449 -0.02584974] \n",
      "\n",
      "After addition word sky and feature vector [ 0.04317668 -0.0389986   0.01506065 -0.03560273 -0.00640112 -0.00782156\n",
      "  0.00584124 -0.00672364 -0.02584449 -0.02584974] \n",
      "\n",
      "word is  is\n",
      "\n",
      "After word is and feature vector [-0.02266192 -0.00251063 -0.03345118 -0.04034328  0.00706533 -0.02801719\n",
      "  0.03403306 -0.04139802  0.01580966  0.00674342] \n",
      "\n",
      "After addition word is and feature vector [ 0.02051476 -0.04150923 -0.01839054 -0.07594601  0.00066421 -0.03583875\n",
      "  0.03987431 -0.04812166 -0.01003483 -0.01910632] \n",
      "\n",
      "word is  blue\n",
      "\n",
      "After word blue and feature vector [-0.0284033   0.01511517  0.01588725 -0.04928996  0.03953237 -0.01278873\n",
      " -0.02903343  0.02796526  0.04616746 -0.03628164] \n",
      "\n",
      "After addition word blue and feature vector [-0.00788854 -0.02639406 -0.00250329 -0.12523597  0.04019659 -0.04862747\n",
      "  0.01084088 -0.0201564   0.03613263 -0.05538796] \n",
      "\n",
      "word is  and\n",
      "\n",
      "word is  sky\n",
      "\n",
      "After word sky and feature vector [ 0.04317668 -0.0389986   0.01506065 -0.03560273 -0.00640112 -0.00782156\n",
      "  0.00584124 -0.00672364 -0.02584449 -0.02584974] \n",
      "\n",
      "After addition word sky and feature vector [ 0.03528815 -0.06539266  0.01255736 -0.1608387   0.03379547 -0.05644903\n",
      "  0.01668212 -0.02688005  0.01028813 -0.0812377 ] \n",
      "\n",
      "word is  is\n",
      "\n",
      "After word is and feature vector [-0.02266192 -0.00251063 -0.03345118 -0.04034328  0.00706533 -0.02801719\n",
      "  0.03403306 -0.04139802  0.01580966  0.00674342] \n",
      "\n",
      "After addition word is and feature vector [ 0.01262622 -0.06790329 -0.02089383 -0.20118198  0.0408608  -0.08446622\n",
      "  0.05071518 -0.06827807  0.0260978  -0.07449428] \n",
      "\n",
      "word is  beautiful\n",
      "\n",
      "After word beautiful and feature vector [-0.02006788 -0.01297529  0.01414281 -0.01726695  0.00837311  0.01428232\n",
      "  0.00198303 -0.03736987  0.03435523  0.02569372] \n",
      "\n",
      "After addition word beautiful and feature vector [-0.00744166 -0.08087858 -0.00675101 -0.21844893  0.04923391 -0.07018389\n",
      "  0.05269822 -0.10564794  0.06045303 -0.04880057] \n",
      "hello\n",
      "\n",
      "\n",
      "finalized feature vector [-0.00124028 -0.01347976 -0.00112517 -0.03640815  0.00820565 -0.01169732\n",
      "  0.00878304 -0.01760799  0.0100755  -0.00813343]\n",
      "\n",
      "word is  the\n",
      "\n",
      "After word the and feature vector [ 0.04696456  0.04587301 -0.03998681 -0.04094198 -0.04281063 -0.03340809\n",
      "  0.01781495  0.02247143  0.03157189  0.03632472] \n",
      "\n",
      "After addition word the and feature vector [ 0.04696456  0.04587301 -0.03998681 -0.04094198 -0.04281063 -0.03340809\n",
      "  0.01781495  0.02247143  0.03157189  0.03632472] \n",
      "\n",
      "word is  beautiful\n",
      "\n",
      "After word beautiful and feature vector [-0.02006788 -0.01297529  0.01414281 -0.01726695  0.00837311  0.01428232\n",
      "  0.00198303 -0.03736987  0.03435523  0.02569372] \n",
      "\n",
      "After addition word beautiful and feature vector [ 0.02689668  0.03289772 -0.02584399 -0.05820893 -0.03443753 -0.01912577\n",
      "  0.01979798 -0.01489844  0.06592713  0.06201844] \n",
      "\n",
      "word is  sky\n",
      "\n",
      "After word sky and feature vector [ 0.04317668 -0.0389986   0.01506065 -0.03560273 -0.00640112 -0.00782156\n",
      "  0.00584124 -0.00672364 -0.02584449 -0.02584974] \n",
      "\n",
      "After addition word sky and feature vector [ 0.07007337 -0.00610088 -0.01078335 -0.09381166 -0.04083864 -0.02694733\n",
      "  0.02563923 -0.02162208  0.04008263  0.0361687 ] \n",
      "\n",
      "word is  is\n",
      "\n",
      "After word is and feature vector [-0.02266192 -0.00251063 -0.03345118 -0.04034328  0.00706533 -0.02801719\n",
      "  0.03403306 -0.04139802  0.01580966  0.00674342] \n",
      "\n",
      "After addition word is and feature vector [ 0.04741144 -0.00861151 -0.04423453 -0.13415494 -0.03377331 -0.05496452\n",
      "  0.05967229 -0.0630201   0.05589229  0.04291212] \n",
      "\n",
      "word is  so\n",
      "\n",
      "word is  blue\n",
      "\n",
      "After word blue and feature vector [-0.0284033   0.01511517  0.01588725 -0.04928996  0.03953237 -0.01278873\n",
      " -0.02903343  0.02796526  0.04616746 -0.03628164] \n",
      "\n",
      "After addition word blue and feature vector [ 0.01900814  0.00650366 -0.02834728 -0.18344489  0.00575906 -0.06775324\n",
      "  0.03063886 -0.03505484  0.10205975  0.00663048] \n",
      "hello\n",
      "\n",
      "\n",
      "finalized feature vector [ 0.00380163  0.00130073 -0.00566946 -0.03668898  0.00115181 -0.01355065\n",
      "  0.00612777 -0.00701097  0.02041195  0.0013261 ]\n",
      "\n",
      "word is  i\n",
      "\n",
      "word is  love\n",
      "\n",
      "word is  blue\n",
      "\n",
      "After word blue and feature vector [-0.0284033   0.01511517  0.01588725 -0.04928996  0.03953237 -0.01278873\n",
      " -0.02903343  0.02796526  0.04616746 -0.03628164] \n",
      "\n",
      "After addition word blue and feature vector [-0.0284033   0.01511517  0.01588725 -0.04928996  0.03953237 -0.01278873\n",
      " -0.02903343  0.02796526  0.04616746 -0.03628164] \n",
      "\n",
      "word is  cheese\n",
      "hello\n",
      "\n",
      "\n",
      "finalized feature vector [-0.0284033   0.01511517  0.01588725 -0.04928996  0.03953237 -0.01278873\n",
      " -0.02903343  0.02796526  0.04616746 -0.03628164]\n",
      "[[ 0.01   0.005 -0.011 -0.042 -0.001 -0.021  0.007  0.001  0.017 -0.005]\n",
      " [-0.001 -0.013 -0.001 -0.036  0.008 -0.012  0.009 -0.018  0.01  -0.008]\n",
      " [ 0.004  0.001 -0.006 -0.037  0.001 -0.014  0.006 -0.007  0.02   0.001]\n",
      " [-0.028  0.015  0.016 -0.049  0.04  -0.013 -0.029  0.028  0.046 -0.036]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Atif Khan\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:24: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "C:\\Users\\Atif Khan\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:26: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "C:\\Users\\Atif Khan\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:30: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n"
     ]
    }
   ],
   "source": [
    " #In gensim 1.0.0+, the `index2word` list of words has moved to the model's `wv` property (a KeyedVectors instance).\n",
    "  #  So replace `model.index2word` with `model.wv.index2word`.\n",
    "#https://stackoverflow.com/questions/31440803/how-to-fetch-vectors-for-a-word-list-with-word2vec\n",
    "\n",
    "\n",
    "my_dict = dict({})\n",
    "#for idx, key in enumerate(model.wv.vocab):\n",
    " #   my_dict[key] = model.wv[key]\n",
    "    # Or my_dict[key] = model.wv.get_vector(key)\n",
    "    # Or my_dict[key] = model.wv.word_vec(key, use_norm=False)\n",
    "    \n",
    "import numpy as np\n",
    "def average_word_vectors(words, model, vocabulary, num_features):\n",
    "    \n",
    "    feature_vector = np.zeros((num_features,),dtype=\"float64\")\n",
    "    nwords = 0.\n",
    "    \n",
    "    for word in words:\n",
    "        print()\n",
    "        print('word is ', word)\n",
    "        if word in vocabulary: \n",
    "            nwords = nwords + 1\n",
    "            #print('word {} and feature vector {} '.format(word,feature_vector))\n",
    "            feature_vector = np.add(feature_vector, model[word])\n",
    "            print()\n",
    "            print('After word {} and feature vector {} '.format(word,model[word]))\n",
    "            print()\n",
    "            print('After addition word {} and feature vector {} '.format(word,feature_vector))\n",
    "            \n",
    "            my_dict[word]=model[word]\n",
    "    \n",
    "    print('hello')\n",
    "    print('')\n",
    "    print('')\n",
    "    #for key,value in my_dict.items():\n",
    "     #   print(key,np.round(value,3))\n",
    "    if nwords:\n",
    "        feature_vector = np.divide(feature_vector, nwords)\n",
    "        print('finalized feature vector', feature_vector)\n",
    "        \n",
    "    return feature_vector\n",
    "    \n",
    "   \n",
    " def averaged_word_vectorizer(corpus, model, num_features):\n",
    "    vocabulary = set(model.wv.index2word)\n",
    "    #print('index2word ',model.wv.index2word)\n",
    "    #print('vocab is ',vocabulary)\n",
    "    \n",
    "    features = [average_word_vectors(tokenized_sentence, model, vocabulary, num_features)\n",
    "                    for tokenized_sentence in corpus]\n",
    "    \n",
    "    return np.array(features)\n",
    "\n",
    "# get averaged word vectors for our training CORPUS\n",
    "\n",
    "avg_word_vec_features = averaged_word_vectorizer(corpus=TOKENIZED_CORPUS, model=model,num_features=10)\n",
    "\n",
    "print (np.round(avg_word_vec_features, 3))\n",
    "#for key,value in my_dict.items():\n",
    " #   print(key,np.round(value,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.03907602  0.01947895 -0.04249009 -0.16617795 -0.00261405 -0.08203557\n",
      "  0.02865582  0.00231503  0.06770452 -0.01906324]\n",
      "\n",
      "[ 0.00976901  0.00486974 -0.01062252 -0.04154449 -0.00065351 -0.02050889\n",
      "  0.00716395  0.00057876  0.01692613 -0.00476581]\n"
     ]
    }
   ],
   "source": [
    "##The purpose and usefulness of Word2vec is to group the vectors of similar words together in vectorspace. \n",
    "#That is, it detects similarities mathematically. Word2vec creates vectors \n",
    "#that are distributed numerical representations of word features, features such as the context of individual words.\n",
    "\n",
    "the= np.array([ 0.04696456,  0.04587301, -0.03998681, -0.04094198, -0.04281063, -0.03340809, 0.01781495,  0.02247143,  0.03157189,  0.03632472]) \n",
    "sky= np.array([ 0.04317668, -0.0389986,   0.01506065, -0.03560273, -0.00640112, -0.00782156, 0.00584124, -0.00672364, -0.02584449, -0.02584974] )\n",
    "\n",
    "isw= np.array([-0.02266192, -0.00251063, -0.03345118, -0.04034328,  0.00706533, -0.02801719, 0.03403306, -0.04139802,  0.01580966,  0.00674342] )\n",
    "\n",
    "blue= np.array([-0.0284033,   0.01511517,  0.01588725, -0.04928996,  0.03953237, -0.01278873, -0.02903343,  0.02796526,  0.04616746, -0.03628164]) \n",
    "\n",
    "total=np.sum((the,sky,isw,blue),axis=0)\n",
    "print(total)\n",
    "print()\n",
    "print(total/4)\n",
    "\n",
    "##[ 0.03907602  0.01947895 -0.04249009 -0.16617795 -0.00261405 -0.08203557\n",
    " # 0.02865582  0.00231503  0.06770452 -0.01906324]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sp' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-38-42f4c708681f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mindices\u001b[0m\u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m4\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m4\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m4\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mdoubled_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mn\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;36m2\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mn\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0mmat_csr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcsr_matrix\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoubled_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindptr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindices\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[0mdisplay_matrix\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmat_csr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtodense\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'sp' is not defined"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "data=np.array([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10])\n",
    "indptr= np.array([ 0, 2, 2, 6, 8, 10])  # row major order indptr refers to rows and indices refers to columnsand viceversa\n",
    "indices= np.array([3, 4, 0, 1, 3, 4, 1, 3, 0, 4])\n",
    "doubled_data = [n * 2 for n in data]\n",
    "mat_csr = sp.csr_matrix((doubled_data, indptr, indices), shape=(5, 5))\n",
    "display_matrix(mat_csr.todense())"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "indptr = np.array([0, 2, 3, 6])\n",
    "indices = np.array([0, 2, 2, 0, 1, 2])# stores row indices for columns\n",
    "data = np.array([1, 2, 3, 4, 5, 6])\n",
    "print(csc_matrix((data, indices, indptr), shape=(3, 3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 1)\t1\n",
      "  (0, 2)\t1\n",
      "  (0, 3)\t1\n",
      "  (0, 6)\t1\n",
      "  (0, 8)\t1\n",
      "  (1, 1)\t2\n",
      "  (1, 3)\t1\n",
      "  (1, 5)\t1\n",
      "  (1, 6)\t1\n",
      "  (1, 8)\t1\n",
      "  (2, 0)\t1\n",
      "  (2, 3)\t1\n",
      "  (2, 4)\t1\n",
      "  (2, 6)\t1\n",
      "  (2, 7)\t1\n",
      "  (2, 8)\t1\n",
      "  (3, 1)\t1\n",
      "  (3, 2)\t1\n",
      "  (3, 3)\t1\n",
      "  (3, 6)\t1\n",
      "  (3, 8)\t1\n",
      "--------------------------\n",
      "[[0 1 1 1 0 0 1 0 1]\n",
      " [0 2 0 1 0 1 1 0 1]\n",
      " [1 0 0 1 1 0 1 1 1]\n",
      " [0 1 1 1 0 0 1 0 1]]\n"
     ]
    }
   ],
   "source": [
    "print(bow_features)\n",
    "#print(bow_features.toarray())\n",
    "print('--------------------------')\n",
    "print(sp.csc_matrix(bow_features, copy=True).toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "We will now compute our document frequencies (DF) for each term based on the number of documents in which it occurs. \n",
    "The following snippet shows how to obtain it from our Bag of Words feature matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-128-ce129c445b2f>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-128-ce129c445b2f>\"\u001b[1;36m, line \u001b[1;32m1\u001b[0m\n\u001b[1;33m    The compressed sparse row (CSR) or compressed row storage (CRS) or Yale format represents a matrix M by three (one-dimensional)\u001b[0m\n\u001b[1;37m                 ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "The compressed sparse row (CSR) or compressed row storage (CRS) or Yale format represents a matrix M by three (one-dimensional)\n",
    "arrays, that respectively contain nonzero values, the extents of rows, and column indices. \n",
    "It is similar to COO, but compresses the row indices, hence the name. \n",
    "This format allows fast row access and matrix-vector multiplications (Mx). \n",
    "The CSR format has been in use since at least the mid-1960s, with the first complete description appearing in 1967.[4]\n",
    "\n",
    "The CSR format stores a sparse m × n matrix M in row form using three (one-dimensional) arrays (A, IA, JA).\n",
    "Let NNZ denote the number of nonzero entries in M. (Note that zero-based indices shall be used here.)\n",
    "Illustration of row-major order compared to column-major order\n",
    "https://en.wikipedia.org/wiki/Sparse_matrix#Compressed_sparse_row_.28CSR.2C_CRS_or_Yale_format.29\n",
    "    http://heydenberk.com/blog/posts/sparse-matrix-representations-in-scipy/\n",
    "The array A is of length NNZ and holds all the nonzero entries of M in left-to-right top-to-bottom (\"row-major\") order.\n",
    "The array IA is of length m + 1. It is defined by this recursive definition:\n",
    "IA[0] = 0\n",
    "IA[i] = IA[i − 1] + (number of nonzero elements on the (i-1)-th row in the original matrix)\n",
    "Thus, the first m elements of IA store the index into A of the first nonzero element in each row of M, and the last element IA[m+1] stores NNZ, the number of elements in A, which can be also thought of as the index in A of first element of a phantom row just beyond the end of the matrix M. The values of the i-th row of the original matrix is read from the elements A[IA[i]] to A[IA[i + 1] − 1] (inclusive on both ends), i.e. from the start of one row to the last index just before the start of the next.[5]\n",
    "The third array, JA, contains the column index in M of each element of A and hence is of length NNZ as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 0 1 1 2 3 5 8]\n"
     ]
    }
   ],
   "source": [
    "#The NumPy diff function “np.diff()” calculates the difference between subsequent values in a NumPy array.\n",
    "\n",
    "import numpy as np\n",
    "# Fibonacci Sequence with first 8 numbers\n",
    "fibs = np.array([0, 1, 1, 2, 3, 5, 8, 13, 21])\n",
    "diff_fibs = np.diff(fibs)\n",
    "print(diff_fibs)\n",
    "# [1 0 1 1 2 3 5 8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0  1  4  6 10 11 12 16 17 21]\n",
      "\n",
      "indices stores row index of each element of data array [2 0 1 3 0 3 0 1 2 3 2 1 0 1 2 3 2 0 1 2 3]\n",
      "\n",
      "data is [1 1 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      "\n",
      " [[0 1 1 1 0 0 1 0 1]\n",
      " [0 2 0 1 0 1 1 0 1]\n",
      " [1 0 0 1 1 0 1 1 1]\n",
      " [0 1 1 1 0 0 1 0 1]]\n",
      "document freq  [1 3 2 4 1 1 4 1 4]\n",
      "   and  document  first  is  one  second  the  third  this\n",
      "0    1         3      2   4    1       1    4      1     4\n",
      "\n",
      "\n",
      "   and  document  first  is  one  second  the  third  this\n",
      "0    2         4      3   5    2       2    5      2     5\n",
      "(4, 9)\n"
     ]
    }
   ],
   "source": [
    "# build the document frequency matrix\n",
    "print(sp.csc_matrix(bow_features, copy=True).indptr)  # poiner (0,1) shows start and end index of 1st column,1 is not included\n",
    "# and in the index array, 0 refers to 1 element in the index array, which is 2 lie in 3rd row of 1st column\n",
    "# poiner (1,4) shows start and end index of 2nd colum column,4 is not included, and covers 3 elements of 2nd col in the index array\n",
    "# and in the index array, 0 refers to 1 element in the index array, which is 2 lie in 3rd row of 1st column\n",
    "\n",
    "#IA[i] = IA[i − 1] + (number of nonzero elements on the (i-1)-th colum in the original matrix) => IA[1]=A[0]+ no of elements in columns 0=0+1=1\n",
    "#=> IA[2]= IA[1]+ no of elements in 1 columns=1+3= 4\n",
    "\n",
    "'''The values of the i-th column of the original matrix is read from the elements A[IA[i]] to A[IA[i + 1] − 1] (inclusive on \n",
    "both ends), i.e. from the start of one row to the last index just before the start of the next'''\n",
    "\"the values of 0 the column   A[IA[0] to A[IA[1]-1]  => A[0] to A[0]  => 1 to 1\"\n",
    "\"the values of 1st column   A[IA[1] to A[IA[2]-1]  => A[1] to A[3]  => 1 to 3\" \n",
    "\n",
    "\n",
    "print('\\nindices stores row index of each element of data array', sp.csc_matrix(bow_features, copy=True).indices)\n",
    "print('\\ndata is', sp.csc_matrix(bow_features, copy=True).data)\n",
    "print('\\n', sp.csc_matrix(bow_features, copy=True).toarray())\n",
    "df = np.diff(sp.csc_matrix(bow_features, copy=True).indptr)\n",
    "print('document freq ',df)\n",
    "display_features([df], feature_names)\n",
    "df = 1 + df # to smoothen idf later\n",
    "'''Remember that we have added 1 to each frequency value to\n",
    "smoothen the idf values later and prevent division-by-zero errors by assuming we have a\n",
    "document (imaginary) that has all the terms once.'''\n",
    "# show document frequencies\n",
    "print()\n",
    "print()\n",
    "display_features([df], feature_names)\n",
    "\n",
    "print(bow_features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "These days, I’m working on some text classification works, and I use gensim=’s =doc2vec function.\n",
    "\n",
    "When using gensim, it shows this warning message: ``` C extension not loaded for Word2Vec, training will be slow. ```\n",
    "\n",
    "I search this on Internet and found that gensim has rewrite some part of the code using `cython` rather than `numpy` to get better performance. A compiler is required to enable this feature.\n",
    "\n",
    "I tried to install mingw and add it into the path, but it’s not working.\n",
    "\n",
    "Finally, I tried to install Visual C++ Build Tools and it works.\n",
    "https://download.microsoft.com/download/5/f/7/5f7acaeb-8363-451f-9425-68a90f98b238/visualcppbuildtools_full.exe\n",
    "\n",
    "If this output a none -1 digit, then it’s fine. ```python3 from gensim.models import word2vec print(word2vec.FAST_VERSION) ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    and  beautiful  blue  cheese    is  love   sky    so   the\n",
      "0  0.00       0.00  0.40    0.00  0.49  0.00  0.49  0.00  0.60\n",
      "1  0.44       0.35  0.23    0.00  0.56  0.00  0.56  0.00  0.00\n",
      "2  0.00       0.43  0.29    0.00  0.35  0.00  0.35  0.55  0.43\n",
      "3  0.00       0.00  0.35    0.66  0.00  0.66  0.00  0.00  0.00\n",
      "\n",
      "[[0.   0.   0.4  0.   0.49 0.   0.49 0.   0.6 ]\n",
      " [0.44 0.35 0.23 0.   0.56 0.   0.56 0.   0.  ]\n",
      " [0.   0.43 0.29 0.   0.35 0.   0.35 0.55 0.43]\n",
      " [0.   0.   0.35 0.66 0.   0.66 0.   0.   0.  ]]\n",
      "\n",
      "{'the': 8, 'sky': 6, 'is': 4, 'blue': 2, 'and': 0, 'beautiful': 1, 'so': 7, 'love': 5, 'cheese': 3}\n",
      "\n",
      "[['the', 'sky', 'is', 'blue'], ['sky', 'is', 'blue', 'and', 'sky', 'is', 'beautiful'], ['the', 'beautiful', 'sky', 'is', 'so', 'blue'], ['i', 'love', 'blue', 'cheese']]\n",
      "\n",
      "  (0, 8)\t0.6031370082211672\n",
      "  (0, 6)\t0.4882913888670788\n",
      "  (0, 4)\t0.4882913888670788\n",
      "  (0, 2)\t0.3992102058196136\n",
      "  (1, 6)\t0.5623513975308212\n",
      "  (1, 4)\t0.5623513975308212\n",
      "  (1, 2)\t0.22987955785181605\n",
      "  (1, 0)\t0.44051606615876376\n",
      "  (1, 1)\t0.3473079263825201\n",
      "  (2, 8)\t0.4320257780944028\n",
      "  (2, 6)\t0.34976210104278727\n",
      "  (2, 4)\t0.34976210104278727\n",
      "  (2, 2)\t0.2859534358554926\n",
      "  (2, 1)\t0.4320257780944028\n",
      "  (2, 7)\t0.5479699188774512\n",
      "  (3, 2)\t0.34618161159873423\n",
      "  (3, 5)\t0.6633846138519129\n",
      "  (3, 3)\t0.6633846138519129\n",
      "[(['the', 'sky', 'is', 'blue'], array([0.  , 0.  , 0.4 , 0.  , 0.49, 0.  , 0.49, 0.  , 0.6 ])), (['sky', 'is', 'blue', 'and', 'sky', 'is', 'beautiful'], array([0.44, 0.35, 0.23, 0.  , 0.56, 0.  , 0.56, 0.  , 0.  ])), (['the', 'beautiful', 'sky', 'is', 'so', 'blue'], array([0.  , 0.43, 0.29, 0.  , 0.35, 0.  , 0.35, 0.55, 0.43])), (['i', 'love', 'blue', 'cheese'], array([0.  , 0.  , 0.35, 0.66, 0.  , 0.66, 0.  , 0.  , 0.  ]))]\n",
      "(10,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "def tfidf_extractor(corpus, ngram_range=(1,1)):\n",
    "\n",
    "    vectorizer = TfidfVectorizer(min_df=1,norm='l2',smooth_idf=True,use_idf=True,ngram_range=ngram_range)\n",
    "\n",
    "    features = vectorizer.fit_transform(corpus)\n",
    "\n",
    "    return vectorizer, features\n",
    "\n",
    "# build tfidf vectorizer and get training corpus feature vectors\n",
    "tfidf_vectorizer, tdidf_features = tfidf_extractor(CORPUS)\n",
    "display_features(np.round(tdidf_features.todense(), 2), feature_names)\n",
    "\n",
    "\n",
    "\n",
    "print()\n",
    "print(np.round(tdidf_features.todense(),2))\n",
    "\n",
    "print()\n",
    "print(tfidf_vectorizer.vocabulary_)\n",
    "print()\n",
    "print(TOKENIZED_CORPUS)\n",
    "print()\n",
    "print(tdidf_features)\n",
    "docs_tfidfs = [(doc, doc_tfidf) for doc, doc_tfidf \n",
    "                   in zip(TOKENIZED_CORPUS, list(np.round(tdidf_features.todense(),2)))]\n",
    "print(docs_tfidfs)\n",
    "\n",
    "a=np.zeros((10,))\n",
    "print(a.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "[[ 0.014  0.007 -0.014 -0.041 -0.005 -0.022  0.009  0.001  0.016 -0.001]\n",
      " [ 0.003 -0.017 -0.004 -0.036  0.005 -0.014  0.014 -0.021  0.004 -0.007]\n",
      " [ 0.006  0.002 -0.007 -0.036 -0.002 -0.013  0.008 -0.008  0.021  0.005]\n",
      " [-0.028  0.015  0.016 -0.049  0.04  -0.013 -0.029  0.028  0.046 -0.036]]\n",
      "\n",
      "\n",
      "[[ 0.011 -0.015  0.015 -0.042  0.014 -0.01  -0.01   0.009  0.007 -0.031]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Atif Khan\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:20: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n"
     ]
    }
   ],
   "source": [
    "def tfidf_wtd_avg_word_vectors(words, tfidf_vector, tfidf_vocabulary, model, num_features):\n",
    "    \n",
    "    #print(tfidf_vector[0, tfidf_vocabulary.get(word)])\n",
    "    print()\n",
    "    word_tfidfs = [tfidf_vector[0, tfidf_vocabulary.get(word)] #tfidf_vector[0,8] refers to entry in the matrix for the\n",
    "                   if tfidf_vocabulary.get(word) \n",
    "                   else 0 for word in words]\n",
    "   # print('hello')\n",
    "    #print(word_tfidfs)\n",
    "    #print()\n",
    "    word_tfidf_map = {word:tfidf_val for word, tfidf_val in zip(words, word_tfidfs)} # mapping of words to TFIDS\n",
    "    \n",
    "    feature_vector = np.zeros((num_features,),dtype=\"float64\") # one dimensioanl array of 10 zeros\n",
    "    vocabulary = set(model.wv.index2word)\n",
    "    #print(vocabulary)\n",
    "    #print('hhhhhhh ',model.wv.index2word)\n",
    "    wts = 0\n",
    "    for word in words:\n",
    "        if word in vocabulary: \n",
    "            word_vector = model[word]\n",
    "            weighted_word_vector = word_tfidf_map[word] * word_vector # multiply word vector with its TFiDf value\n",
    "            wts = wts + word_tfidf_map[word]            # sum up the word TFIDf\n",
    "            feature_vector = np.add(feature_vector, weighted_word_vector)\n",
    "    if wts:\n",
    "        feature_vector = np.divide(feature_vector, wts)\n",
    "        \n",
    "    return feature_vector\n",
    "\n",
    "\n",
    "\n",
    "def tfidf_weighted_averaged_word_vectorizer(corpus, tfidf_vectors, \n",
    "                                   tfidf_vocabulary, model, num_features):\n",
    "                                       \n",
    "    docs_tfidfs = [(doc, doc_tfidf) \n",
    "                   for doc, doc_tfidf \n",
    "                   in zip(corpus, tfidf_vectors)]# combine 1st document with thier TF-IDS and so on\n",
    "    \n",
    "    features = [tfidf_wtd_avg_word_vectors (tokenized_sentence, tfidf, tfidf_vocabulary, model, num_features)\n",
    "                    for tokenized_sentence, tfidf in docs_tfidfs]\n",
    "   # print('here are finalized feature set for four documents ',features)\n",
    "    \n",
    "    return np.array(features) \n",
    "\n",
    "\n",
    "\n",
    "corpus_tfidf = tdidf_features\n",
    "vocab = tfidf_vectorizer.vocabulary_  # dictionary of unique words along with column index\n",
    "\n",
    "#{'the': 8, 'sky': 6, 'is': 4, 'blue': 2, 'and': 0, 'beautiful': 1, 'so': 7, 'love': 5, 'cheese': 3}\n",
    "\n",
    "wt_tfidf_word_vec_features = tfidf_weighted_averaged_word_vectorizer(corpus=TOKENIZED_CORPUS,\n",
    "                                                                     tfidf_vectors=corpus_tfidf,\n",
    "                                                                     tfidf_vocabulary=vocab,\n",
    "                                                                     model=model, \n",
    "                                                                     num_features=10)\n",
    "print(np.round(wt_tfidf_word_vec_features, 3))\n",
    "print()\n",
    "\n",
    "nd_wt_tfidf_word_vec_features = tfidf_weighted_averaged_word_vectorizer(corpus=tokenized_new_doc,\n",
    "                                                                     tfidf_vectors=corpus_tfidf,\n",
    "                                                                     tfidf_vocabulary=vocab,\n",
    "                                                                     model=model, \n",
    "                                                                     num_features=10)\n",
    "print(np.round(nd_wt_tfidf_word_vec_features, 3))  \n",
    "                                                     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import word2vec \n",
    "print(word2vec.FAST_VERSION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
